# Databricks Asset Bundle configuration for Parallel Web Search Agents with Lakeflow

bundle:
  name: lakeflow_parallel_web_search_agents

# Include additional configuration files
include:
  - resources/*.yml

# Variables that can be referenced in the bundle
variables:
  catalog_name:
    description: Unity Catalog name
    default: this_is_for_tahir_only

  schema_name:
    description: Schema name within the catalog
    default: robocars

# Workspace configuration
workspace:
  host: https://dbc-08fc9045-faef.cloud.databricks.com/

# Target environments
targets:
  dev:
    mode: development
    default: true
    workspace:
      host: https://dbc-08fc9045-faef.cloud.databricks.com/
    variables:
      catalog_name: this_is_for_tahir_only
      schema_name: robocars

  prod:
    mode: production
    workspace:
      host: https://dbc-08fc9045-faef.cloud.databricks.com/
      # Uncomment and set the root path for production deployments
      # root_path: /Workspace/Production/lakeflow_parallel_web_search_agents
    variables:
      catalog_name: this_is_for_tahir_only
      schema_name: robocars
    # Add production-specific settings like approval requirements
    run_as:
      # service_principal_name: "sp-production-jobs"
      user_name: "tahir.fayyaz@databricks.com"

# Resources
resources:
  jobs:
    feature_news_update_workflow:
      name: "[${bundle.target}] Feature News Update Workflow"
      description: |
        Lakeflow job that orchestrates parallel web search for self-driving car features.
        This workflow:
        1. Retrieves all feature IDs from the Unity Catalog table
        2. Processes each feature in parallel using Claude API with web search
        3. Updates the table with new information
        4. Creates SCD Type 2 historical records

      # Using serverless compute - no job_clusters needed

      tasks:
        # Task 1: Get all feature IDs
        - task_key: get_feature_ids
          notebook_task:
            notebook_path: ./src/02_get_feature_ids.py
            source: WORKSPACE
          timeout_seconds: 600
          libraries:
            - pypi:
                package: anthropic

        # Task 2: Process each feature in parallel using for_each
        - task_key: process_features_parallel
          depends_on:
            - task_key: get_feature_ids
          for_each_task:
            inputs: "{{tasks.get_feature_ids.values.feature_ids}}"
            concurrency: 10  # Process up to 10 features in parallel
            task:
              task_key: process_single_feature
              notebook_task:
                notebook_path: ./src/03_process_feature_with_claude.py
                source: WORKSPACE
                base_parameters:
                  core_feature_id: "{{input}}"
              timeout_seconds: 300
              libraries:
                - pypi:
                    package: anthropic

        # Task 3: Create SCD Type 2 table from CDF
        - task_key: create_scd_type2
          depends_on:
            - task_key: process_features_parallel
          notebook_task:
            notebook_path: ./src/04_create_scd_type2_table.py
            source: WORKSPACE
          timeout_seconds: 900

      # Schedule - run daily at 2 AM UTC
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"
        timezone_id: "UTC"
        pause_status: PAUSED  # Start paused, user can unpause when ready

      # Email notifications
      email_notifications:
        on_success:
          - tahir.fayyaz@databricks.com
        on_failure:
          - tahir.fayyaz@databricks.com
        on_duration_warning_threshold_exceeded:
          - tahir.fayyaz@databricks.com

      # Timeout for entire workflow
      timeout_seconds: 7200  # 2 hours

      # Max concurrent runs
      max_concurrent_runs: 1

      # Tags for organization
      tags:
        project: lakeflow_parallel_web_search
        team: data_engineering
        cost_center: analytics

      # Run as configuration
      run_as:
        user_name: tahir.fayyaz@databricks.com

# Permissions for the bundle resources
permissions:
  - level: CAN_MANAGE
    user_name: tahir.fayyaz@databricks.com
